= JWT Microbenchmark Optimization Plan
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:
:source-highlighter: highlight.js

== Action Items Checklist

=== Immediate Priority - Claims Validation Investigation üî¥ **CRITICAL PERFORMANCE ISSUES IDENTIFIED**
- [x] ‚úÖ **COMPLETED** Deep analysis of source code using ULTRATHINK
- [x] ‚úÖ **COMPLETED** Identified 27+ time system calls causing 4,813x variance
- [x] ‚úÖ **COMPLETED** Implemented ValidationContext to cache current time
- [x] ‚úÖ **COMPLETED** Eliminated 3 critical OffsetDateTime.now() calls from validation path
- [ ] Add JFR instrumentation to measure time operation variance under concurrent load
- [ ] Profile complete 27+ time system calls distribution under 200-thread load

==== **ULTRATHINK Analysis Results** - Root Cause Investigation Findings

**Critical Performance Bottlenecks Identified** (Source Code Analysis):

1. **üî¥ SYNCHRONOUS TIME OPERATIONS** - **Primary Suspect for 4,813x Variance** ‚ö†Ô∏è **MUCH WORSE THAN INITIALLY THOUGHT**
   
   **Critical Path Operations (ALWAYS executed)**:
   - **TokenContent.java:163**: `OffsetDateTime.now()` in `isExpired()` method - **EVERY token validation**
   - **ExpirationValidator.java:92**: `OffsetDateTime.now().plusSeconds(60)` for nbf validation - **EVERY token validation**
   - **ExpirationValidator.java:65, 97**: `OffsetDateTime.now()` in exception messages - **On validation failures**
   - **Total**: **3-4 OffsetDateTime.now() calls per token validation** (minimum 3, always executed)
   
   **Metrics Operations (When enabled - very common)**:
   - **TokenValidator.java**: **18 System.nanoTime() calls** per validation (lines 308, 325, 332, 344, 351, 356, 363, 377, 392, 397, 404, 410, 417, 434, 446, 459, 467, 474)
   - **BearerTokenProducer.java**: **6+ System.nanoTime() calls** per HTTP request (Quarkus integration)
   - **Total**: **24+ additional system calls when metrics enabled**
   
   **Problem Scale**:
   - **Minimum**: 3 OffsetDateTime.now() calls per validation (critical path)
   - **With metrics**: 27+ total time system calls per token validation  
   - **Under 200-thread load**: System call contention causes extreme variance
   - **Evidence**: Each OffsetDateTime.now() can vary from microseconds to milliseconds under load

2. **üü° MEMORY ALLOCATION HOTSPOTS** - **Secondary Performance Impact**
   - **TokenBuilder.java:111**: `new HashMap<>()` for every token's claim extraction
   - **MandatoryClaimsValidator.java:83**: `new TreeSet<>()` for missing claims collection
   - **ClaimValue.java:26**: `new ArrayList<>()` for STRING_LIST claim types
   - **MandatoryClaimsValidator.java:153-175**: `StringBuilder` allocation in error handling
   - **Impact**: GC pressure causing tail latency spikes under concurrent load

3. **üü° COLLECTION OPERATIONS UNDER LOAD**
   - **AudienceValidator.java:129-137**: Stream operations with `anyMatch()` on collections
   - **TokenBuilder.java:114-132**: Map lookups and iterations during claim extraction
   - **Various validators**: Set containment checks that scale with memory pressure
   - **Impact**: Performance degradation under high concurrency (200 threads)

4. **üü° STRING PROCESSING OVERHEAD**
   - **Error message formatting**: Multiple string concatenations and formatting operations
   - **Log message preparation**: String formatting even when logging disabled at runtime
   - **ClaimName operations**: String-to-enum lookups and mappings
   - **Impact**: CPU overhead and string object allocation

5. **üü° OPTIONAL CHAIN OPERATIONS**
   - **TokenContent interface**: Multiple `Optional.map()` chains creating intermediate objects
   - **ClaimName.fromString()**: Optional-based lookups with object creation
   - **Impact**: Additional object allocation and method call overhead

**Specific Problem Locations** - **COMPREHENSIVE TIME OPERATIONS AUDIT**:

```java
// PRIMARY BOTTLENECK - Critical Path Time Operations (ALWAYS executed)
// TokenContent.java:163
// return getExpirationTime().isBefore(OffsetDateTime.now());  - EVERY validation

// ExpirationValidator.java:92  
// if (notBefore.get().isAfter(OffsetDateTime.now().plusSeconds(CLOCK_SKEW_SECONDS)))  - EVERY validation

// ExpirationValidator.java:65, 97 (on validation failures)
// "Current time: " + OffsetDateTime.now()  - Exception messages

// SECONDARY BOTTLENECK - Metrics Time Operations (When enabled - common)
// TokenValidator.java: 18 System.nanoTime() calls per validation
// BearerTokenProducer.java: 6+ System.nanoTime() calls per request

// MEMORY ALLOCATION BOTTLENECKS  
// TokenBuilder.java:111 - new HashMap<>();  per token
// MandatoryClaimsValidator.java:83 - new TreeSet<>();  per validation
```

**Total Time System Calls Per Token Validation**:
- **Minimum (critical path)**: 3 OffsetDateTime.now() calls  
- **With metrics enabled**: 27+ total time system calls
- **Root cause of 4,813x variance**: System call contention under 200-thread load

**Performance Impact Analysis**:
- **P50 (0.020ms)**: Fast path when no GC pressure, cached time operations
- **P99 (96.253ms)**: Extreme tail latency when GC pressure + time operation variance combine
- **4,813x variance**: Indicates combination of GC pauses + synchronous system calls
- **200-thread load**: Amplifies both memory pressure and time operation contention

**Next Investigation Priority** - **RE-EVALUATE: True Root Cause of P99 Variance**:

**CRITICAL (Must fix - causes 4,813x variance)**:
1. **Eliminate OffsetDateTime.now() in critical path** ‚úÖ **COMPLETED**
   - [x] Cached current time at validation start via ValidationContext
   - [x] Removed OffsetDateTime.now() from TokenContent.isExpired() 
   - [x] Removed OffsetDateTime.now() from ExpirationValidator.validateNotBefore()
   
2. **Optimize metrics System.nanoTime() calls** (24+ calls when enabled)
   - [ ] Batch timing measurements to reduce system call frequency
   - [ ] Consider disabling metrics in performance-critical scenarios
   - [ ] Use single start time, calculate deltas instead of multiple nanoTime() calls

**HIGH PRIORITY (Secondary optimization)**:
3. **Object pool for HashMap allocation** - Reuse claim maps to reduce GC pressure  
4. **Profile complete time operation distribution** - Measure all 27+ calls under load

=== High Priority - Tail Latency Analysis  
- [ ] Enable GC logging in microbenchmarks (-XX:+PrintGC)
- [ ] Profile thread contention in validation pipeline
- [ ] Compare single-thread vs 200-thread performance patterns
- [ ] Identify shared resource access causing contention
- [ ] Analyze object creation hotspots

=== Medium Priority - Signature Validation
- [ ] Profile RSA operations under concurrent load (default JDK provider)
- [ ] Analyze BigInteger.modPow performance characteristics
- [ ] Compare signature validation across different thread counts
- [ ] Investigate 187x P50-to-P99 variance root cause

=== Validation
- [ ] Re-run benchmarks: `./mvnw --no-transfer-progress clean verify -pl cui-jwt-benchmarking -Pbenchmark`
- [ ] Collect 100,000+ samples for statistical significance
- [ ] Validate claims validation P99 reduction
- [ ] Confirm overall validation latency improvement
- [ ] Document optimization techniques applied

== Performance Analysis Summary

**Module**: `cui-jwt-benchmarking` - JMH microbenchmarks, isolated JWT library performance

**LATEST RESULTS** - After ValidationContext Implementation (65,536 samples, 200 threads):
- **Throughput**: 57,106 ops/sec (¬±166K variance) - Previously 71,151
- **Average Latency**: 2.9ms per operation - Previously 2.6ms

=== ‚ö†Ô∏è **UNEXPECTED RESULT**: Claims Validation P99 Increased from 96ms to 105ms

**Before Optimization** (Previous Run):
|===
| Validation Step | P50 | P95 | P99 | P99/P50 Ratio
| **Claims Validation** | 0.020ms | 2.545ms | **96.253ms** | **4,813x** üî¥
| Complete Validation | 0.142ms | 7.689ms | 138.981ms | 979x
| Signature Validation | 0.083ms | 0.144ms | 15.562ms | 187x
| Token Parsing | 0.012ms | 0.021ms | 0.143ms | 12x ‚úÖ
| Header Validation | 0.001ms | 0.002ms | 0.003ms | 3x ‚úÖ
|===

**After ValidationContext Optimization** (Current Run):
|===
| Validation Step | P50 | P95 | P99 | P99/P50 Ratio | Improvement
| **Claims Validation** | 0.029ms | 4.796ms | **104.936ms** | **3,618x** | ‚ö†Ô∏è P99 worse by 9%
| Complete Validation | 0.173ms | 18.358ms | 146.951ms | 850x | P99 worse by 6%
| Signature Validation | 0.093ms | 0.165ms | 13.532ms | 145x | P99 improved by 13%
| Token Parsing | 0.014ms | 0.027ms | 0.080ms | 6x ‚úÖ | P99 improved by 44%
| Header Validation | 0.001ms | 0.002ms | 0.003ms | 3x ‚úÖ | No change
| Token Building | 0.024ms | 0.107ms | 30.346ms | 1,264x | New metric
|===

**Performance Degradation Analysis**:
- **Claims validation P99 WORSENED** from 96ms to 105ms (+9%) despite removing 3 OffsetDateTime.now() calls
- **P50 latency WORSENED** from 0.020ms to 0.029ms (+45%)
- **Complete validation P99 WORSENED** from 139ms to 147ms (+6%)
- **Throughput DECREASED** from 71,151 to 57,106 ops/sec (-20%)

**Root Cause Hypothesis for P99 Degradation**: 
1. **Run-to-run variance**: P99 latencies can vary significantly between benchmark runs due to:
   - GC timing differences
   - OS scheduling variations under 200-thread load
   - CPU thermal throttling
   - Background system activity
2. **Benchmark methodology**: Only 3 iterations with 4s measurement may not capture true P99
3. **ValidationContext implementation**: While removing 3 time calls, we added:
   - Object allocation per validation
   - Additional method parameter passing through entire pipeline
   - Constructor overhead for ValidationContext
4. **The optimization addressed the wrong bottleneck**: The 4,813x variance likely comes from:
   - GC pauses under memory pressure
   - Thread contention on shared resources
   - Not from the 3 OffsetDateTime.now() calls themselves

=== Error Handling Performance

**Fast-fail scenarios** (Œºs/op averages - Latest Run):
- Invalid signature: 673-935Œºs (fastest detection)
- Malformed tokens: 673-705Œºs  
- Valid tokens: 2,475Œºs (stable)
- Expired tokens: 2,803-2,970Œºs (slight improvement)
- Mixed tokens: 2,021-2,705Œºs

**Observation**: Error percentage (0% vs 50%) has minimal performance impact. Performance remains consistent.

== Root Cause Investigation Focus

=== Claims Validation Bottleneck Analysis

**Hypothesis**: 4,813x P50-to-P99 variance indicates:
1. **Expensive operations**: Complex claim validation logic
2. **Concurrency issues**: Shared resource contention
3. **Memory pressure**: Object allocation during validation
4. **Time calculations**: Date/time operations under load

**Investigation Required**:
- Profile specific claim validation methods
- Identify which claims cause high latency
- Analyze memory allocation patterns
- Check for blocking operations

=== Tail Latency Pattern Analysis

**Problem**: Extreme variance across all validation steps suggests systemic issues:
- Claims: 4,813x variance
- Complete validation: 979x variance  
- Signature validation: 187x variance

**Likely Causes**:
- GC pressure from object allocation
- Thread contention under 200-thread load
- Resource exhaustion at high concurrency

== Dismissed Optimization Approaches

=== JWT Token Caching
**Status:** ‚ùå DISMISSED - Processing time too high, caching won't solve core issue

**Reason:** With P99 latencies of 96ms for claims validation and 15ms for signature validation, caching cannot address the fundamental performance bottlenecks. The extreme variance (4,813x for claims validation) indicates algorithmic or concurrency issues that require direct optimization rather than avoidance through caching.

=== ES256 Algorithm Migration  
**Status:** ‚ùå DISMISSED - Integration tests use RS256, microbenchmarks follow suit

**Reason:** Integration test infrastructure is built around RS256. Microbenchmarks measure the same algorithm to ensure consistency. ES256 vs RS256 performance comparison is out of scope for core library optimization.

=== Hardware-Specific Optimizations
**Status:** ‚ùå DISMISSED - Focus on algorithmic improvements

**Reason:** CPU-specific optimizations (AES-NI, ARM crypto extensions) compromise portability and don't address the claims validation bottleneck which appears to be algorithmic rather than cryptographic.

=== Alternative JCA Providers in Microbenchmarks
**Status:** ‚ùå DISMISSED - Microbenchmarks use default JDK providers for consistency

**Reason:** BouncyCastle and other providers are integration test concerns. Microbenchmarks focus on core library performance with standard JDK providers to isolate library-specific bottlenecks.

=== Extended Measurement Duration
**Status:** ‚ùå DISMISSED - 4-second measurement sufficient for trend identification

**Reason:** Current setup provides 65,075 samples with clear P99 bottleneck identification. Extending measurement time won't change the 4,813x variance pattern in claims validation - investigation and optimization needed instead.

== Technical Context

**Microbenchmark Setup**:
- JMH 1.37, Java 21.0.7
- 200 threads, 3 iterations, 4s measurement, 1s warmup
- Default JDK cryptographic providers (no BouncyCastle)

**vs Integration Tests**:
- Microbenchmarks: 2.6ms average (pure library)
- Integration tests: 186.6ms P95 (with framework)
- **66x difference** = 97% framework overhead

**Focus**: Core library optimization separate from infrastructure optimization.