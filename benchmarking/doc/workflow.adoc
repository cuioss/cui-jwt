= Benchmark Workflow
:source-highlighter: highlight.js

Complete workflow guide for running and processing benchmarks.

== CI/CD Pipeline

The benchmark processing is automatically triggered in GitHub Actions:

=== Triggers

* **Pull Request Merge** - Runs on merged PRs to main branch
* **Tag Push** - Runs on all tag pushes
* **Manual Dispatch** - Can be triggered manually

=== Workflow Steps

1. **Environment Setup**
   - Checkout code with full history
   - Set up JDK 21 with Maven cache
   - Build cui-jwt-validation module

2. **Micro Benchmarks**
   - Run JMH benchmarks using `benchmark` profile
   - Automatically generates artifacts in `target/benchmark-results`
   - Creates performance badges, reports, and metrics

3. **WRK Load Testing Benchmarks**
   - Start Quarkus and Keycloak containers
   - Run WRK HTTP load tests
   - Collect Prometheus metrics
   - Automatically generates WRK-specific artifacts

4. **Artifact Combination**
   - Merge artifacts from both benchmark modules
   - Create unified GitHub Pages structure
   - Add metadata and timestamps

5. **Deployment**
   - Deploy combined results to cuioss.github.io
   - Results available at `cui-jwt/benchmarks`

== Local Development

=== Running Benchmarks Locally

[source,bash]
----
# Full micro benchmark suite
./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark

# Quick test (1 iteration)
./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark \
  -Djmh.iterations=1 -Djmh.warmupIterations=1

# With profiling
./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark-jfr

# WRK load testing benchmarks
./mvnw clean verify -pl benchmarking/benchmark-integration-wrk -Pbenchmark
----

=== Viewing Generated Artifacts

After running benchmarks, artifacts are automatically generated:

[source,bash]
----
# View micro benchmark artifacts
ls -la benchmarking/benchmark-library/target/benchmark-results/

# View WRK benchmark artifacts
ls -la benchmarking/benchmark-integration-wrk/target/benchmark-results/

# Serve library benchmark reports for viewing
cd benchmarking/scripts
./serve-reports.sh library
# Open http://localhost:8080

# Or serve WRK benchmark reports
./serve-reports.sh wrk
# Open http://localhost:8080
----

== Generated Artifacts

=== Structure

Each benchmark module generates a consistent artifact structure:

[source]
----
target/benchmark-results/
├── badges/                      # Shields.io compatible badges
│   ├── performance-badge.json   # Overall performance score
│   ├── trend-badge.json         # Trend indicator
│   └── last-run-badge.json      # Timestamp badge
├── data/                        # Metrics data
│   ├── metrics.json            # Combined metrics
│   └── *-metrics.json          # Individual benchmark metrics
├── gh-pages-ready/             # GitHub Pages deployment structure
│   ├── index.html              # Main landing page
│   ├── trends.html             # Historical trends
│   ├── api/                    # JSON API endpoints
│   └── badges/                 # Badge files
├── benchmark-summary.json       # Overall summary with quality gates
└── *-benchmark-result.json     # Raw JMH results
----

=== Badge Types

* **Performance Score** - Weighted composite score (0-100)
* **Trend** - Up/down/stable indicator with percentage change
* **Last Run** - Timestamp of most recent benchmark
* **WRK Performance** - WRK load testing specific score

=== Quality Gates

Each benchmark run evaluates:

* **Throughput** - Minimum operations per second
* **Latency** - Maximum response times (P50, P90, P99)
* **Regression** - Performance change from baseline
* **Overall Status** - PASS/FAIL based on all gates

== API Endpoints

The generated artifacts include JSON API endpoints:

* `api/latest.json` - Latest benchmark results
* `api/metrics.json` - Detailed metrics breakdown
* `api/status.json` - Current quality gate status
* `api/benchmarks.json` - List of all benchmarks

== Performance Requirements

See link:performance-requirements.adoc[Performance Requirements] for specific targets and thresholds.

== Troubleshooting

=== No Artifacts Generated

Check that:
- Benchmarks completed successfully
- No compilation errors in cui-benchmarking-common
- Correct profile used (`-Pbenchmark`)

=== Quality Gates Failing

Review:
- Performance thresholds in `SummaryGenerator`
- Baseline comparison data availability
- Resource constraints during benchmark run

=== Local Viewing Issues

Ensure:
- Python installed for `serve-reports.sh` (Python 3 or 2)
- Port 8080 available (or use `./serve-reports.sh stop` to stop existing server)
- Generated reports exist in `cui-benchmarking-common/target/benchmark-reports-preview/`