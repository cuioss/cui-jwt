= JWT Performance Scoring System
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:
:source-highlighter: highlight.js


== Overview

A weighted performance score combining throughput, latency, and error resilience into a single metric.

== Formula

[source,text]
----
Performance Score = (Throughput_Score × 0.5) + (Latency_Score × 0.5)

Where:
- Throughput_Score = Throughput ÷ 100        // 10000 ops/s = score 100 (uncapped)
- Latency_Score = 100 ÷ Latency_ms          // 1 ms = score 100 (uncapped)
- Throughput = Average operations/second from benchmarks
- Latency_ms = Average latency in milliseconds

Note: Scores are not capped, allowing exceptional performance to exceed 100.
Final scores are rounded to the nearest integer.
----

== Metrics

=== Throughput (50% weight)

* Measures: Average operations per second across all throughput benchmarks
* Score normalized: 10,000 ops/s = 100 points
* Higher is better, no upper limit

=== Latency (50% weight)

* Measures: Average latency across all latency benchmarks
* Score normalized: 1 ms = 100 points (inverse relationship)
* Lower is better, no upper limit on score

== Score Interpretation

[cols="1,1,2", options="header"]
|===
|Score Range |Grade |Description

|≥ 95
|A+
|Exceptional performance - exceeds all targets

|90-94
|A
|Excellent performance - high-scale production ready

|75-89
|B
|Good performance - suitable for most production scenarios

|60-74
|C
|Adequate performance - typical applications

|40-59
|D
|Below average - optimization recommended

|< 40
|F
|Poor performance - optimization required
|===

== Example Calculation

[source,text]
----
Integration Benchmark:
Average Throughput: 5,847 ops/sec
Average Latency: 1.17 ms

Throughput_Score = 5847 ÷ 100 = 58.47
Latency_Score = 100 ÷ 1.17 = 85.47

Raw Score = (58.47 × 0.5) + (85.47 × 0.5)
          = 29.24 + 42.74
          = 71.97

Performance Score = round(71.97) = 72 (Grade C)

Micro Benchmark:
Average Throughput: 83,909 ops/sec
Average Latency: 0.35 ms

Throughput_Score = 83909 ÷ 100 = 839.09
Latency_Score = 100 ÷ 0.35 = 285.71

Raw Score = (839.09 × 0.5) + (285.71 × 0.5)
          = 419.55 + 142.86
          = 562.41

Performance Score = round(562.41) = 562 (Grade A+, exceptional performance)
----

== Badge Format

[source,text]
----
Performance Score: Grade A (45k ops/s, 0.15ms)
                      ↑      ↑            ↑
                      |      |            └─ Average validation time
                      |      └─ Throughput (rounded)
                      └─ Grade based on score
----

== Trend Calculation

Performance trends use an **Exponentially Weighted Moving Average (EWMA)** approach to detect meaningful changes while smoothing out noise:

[source,text]
----
Trend Change % = ((Current_Score - EWMA_Baseline) / EWMA_Baseline) × 100

Where:
EWMA_Baseline = Σ(score_i × weight_i) / Σ(weight_i)

Weights: Exponentially decreasing with λ = 0.25
  - Most recent:  weight = λ^0 = 1.0
  - 2nd recent:   weight = λ^1 = 0.25
  - 3rd recent:   weight = λ^2 = 0.0625
  - And so on...

Trend Direction:
  - Up (green):    change > +2%
  - Stable (blue): -2% ≤ change ≤ +2%
  - Down (red):    change < -2%
----

**Rationale**: EWMA weights recent results heavily while still considering historical context. This prevents false positives from single anomalous runs while quickly detecting real performance shifts. The λ=0.25 decay factor is standard for performance monitoring (NIST guidelines).

== Usage

* **Regression Detection**: Track performance over releases
* **Optimization Tracking**: Measure improvement impact
* **Capacity Planning**: Understand performance characteristics

== Limitations

* Environment dependent (hardware, JVM settings)
* Based on synthetic test tokens
* Single library performance only

== Best Practices

1. Focus on trends over absolute values
2. Use consistent test environments
3. Run multiple iterations for accuracy
4. Consider context when interpreting results