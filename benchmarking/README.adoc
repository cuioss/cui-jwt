= CUI JWT Benchmarking
:toc:
:toclevels: 2

Comprehensive performance testing infrastructure for JWT validation.

== Overview

The benchmarking infrastructure provides:

* **Automated Artifact Generation** - Badges, reports, and metrics generated automatically
* **Dual Benchmark Types** - Micro benchmarks for library performance, integration benchmarks for real-world scenarios
* **GitHub Pages Integration** - Ready-to-deploy artifacts for visualization
* **Performance Tracking** - Historical trends and quality gates
* **Common Infrastructure** - Shared utilities in cui-benchmarking-common for consistency

== Modules

|===
|Module |Purpose |Execution Time

|link:cui-benchmarking-common/[cui-benchmarking-common]
|Shared infrastructure and utilities
|N/A (library)

|link:benchmark-library/[benchmark-library]
|JMH micro-benchmarks for JWT library
|< 10 minutes

|link:benchmark-integration-wrk/[benchmark-integration-wrk]
|WRK-based HTTP load testing
|< 5 minutes
|===

== Quick Start

=== Run Benchmarks

[source,bash]
----
# Library micro-benchmarks
./mvnw verify -pl benchmarking/benchmark-library -Pbenchmark

# WRK load testing benchmarks (requires Docker)
./mvnw verify -pl benchmarking/benchmark-integration-wrk -Pbenchmark
----

=== Run with Profiling

[source,bash]
----
# Library benchmarks with JFR profiling
./mvnw verify -pl benchmarking/benchmark-library -Pbenchmark-jfr
----

=== View Results Locally

[source,bash]
----
cd benchmarking/scripts
./serve-reports.sh library  # For library benchmarks
# or
./serve-reports.sh wrk      # For WRK benchmarks
# Open http://localhost:8080 in your browser
----

See link:doc/local-testing.adoc[Local Testing Guide] for detailed instructions.

== Benchmark Types

=== Micro Benchmarks (benchmark-library)

Located in `benchmarking/benchmark-library`:

* `SimpleCoreValidationBenchmark` - Core JWT validation performance
* `SimpleErrorLoadBenchmark` - Error handling and resilience testing
* Automatic generation of performance badges and reports
* Quality gates for throughput and latency

See link:benchmark-library/README.adoc[benchmark-library README] for details.

=== Load Testing Benchmarks (benchmark-integration-wrk)

Located in `benchmarking/benchmark-integration-wrk`:

* External HTTP load testing using WRK
* JWT validation endpoint stress testing
* Health check endpoint baseline comparison
* System-level CPU and memory monitoring
* Cross-platform monitoring support (macOS/Linux)

See link:benchmark-integration-wrk/README.adoc[benchmark-integration-wrk README] for details.

=== Common Infrastructure (cui-benchmarking-common)

Shared infrastructure providing benchmark framework, metrics collection, and artifact generation.

See link:cui-benchmarking-common/README.adoc[cui-benchmarking-common README] for details.

== Results and Artifacts

=== Generated Structure

[source]
----
target/benchmark-results/
├── badges/                      # Shields.io compatible badges
│   ├── performance-badge.json   # Overall performance score
│   ├── trend-badge.json         # Trend indicator
│   └── last-run-badge.json      # Timestamp badge
├── data/                        # Metrics data
│   ├── metrics.json            # Combined metrics
│   └── *-metrics.json          # Individual benchmark metrics
├── reports/                     # HTML reports
├── gh-pages-ready/             # GitHub Pages deployment structure
│   ├── index.html              # Main landing page
│   ├── trends.html             # Historical trends
│   ├── api/                    # JSON API endpoints
│   └── badges/                 # Badge files
├── benchmark-summary.json       # Overall summary with quality gates
└── *-benchmark-result.json     # Raw JMH/WRK results
----

=== Quality Gates

Each benchmark run evaluates:

* **Throughput thresholds** - Minimum operations per second
* **Latency targets** - Maximum response times
* **Regression detection** - Performance degradation from baseline
* **Overall scoring** - Weighted composite performance score

See link:doc/performance-scoring.adoc[Performance Scoring] for scoring methodology.

=== CI/CD Integration

The GitHub Actions workflow automatically:

1. Runs both micro and integration benchmarks
2. Collects generated artifacts from each module
3. Combines results into a unified GitHub Pages structure
4. Deploys to `cuioss.github.io/OAuth-Sheriff/benchmarks`

See link:doc/workflow.adoc[Benchmark Workflow] for the complete process.

== Documentation

=== Core Documentation

* link:doc/Architecture.adoc[Module Architecture] - Comprehensive module architecture, responsibilities, and code placement guidelines
* link:doc/workflow.adoc[Benchmark Workflow] - Complete workflow guide for running and processing benchmarks
* link:doc/local-testing.adoc[Local Testing Guide] - How to test visualizations locally
* link:doc/performance-scoring.adoc[Performance Scoring] - Weighted metrics methodology
* link:doc/benchmark-metrics.adoc[Benchmark Metrics System] - CPU, memory, and application metrics collection
* link:doc/JFR-Instrumentation.adoc[JFR Instrumentation] - Variance analysis guide
* xref:../doc/Requirements.adoc#CUI-JWT-9[Performance Requirements] - Specific performance targets and verification criteria

=== Performance Analysis

* link:doc/Analysis-10.2025.adoc[Resource Monitoring Enhancement Analysis (October 2025)] - CPU and RAM utilization monitoring implementation and comparative analysis
* link:benchmark-library/doc/Analysis-08.2025.adoc[Benchmark Analysis (August 2025)] - Performance metrics and optimization insights

== Development

When adding new benchmarks or utilities:

1. Review link:doc/Architecture.adoc[Architecture.adoc] for code placement guidelines
2. Follow the decision tree to determine the correct module
3. Use existing patterns and base classes
4. Ensure proper metrics collection and reporting

== Prerequisites

* Java 21+
* Docker (for integration tests and WRK load testing)
* Available ports: 10443, 1443 (for integration tests)

== Published Results

Live benchmark results are available at:

* https://cuioss.github.io/OAuth-Sheriff/benchmarks/
